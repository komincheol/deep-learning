{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.3 Splitting datasets one feature at a time: decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사결정 트리 (Decision Trees)\n",
    "\n",
    "* 의사결정 트리는 마치 스무고개 게임처럼 동작한다.\n",
    "\n",
    "\n",
    "* Decision Tree Flowchart (그림 3.1 p.49)\n",
    "  - Decision Block (의사결정 블록, 사각형)\n",
    "  - Terminal Block (단말 블록, 타원형)\n",
    "  - Branch (가지)\n",
    "  \n",
    "\n",
    "* 장점\n",
    "  - 적은 계산 비용\n",
    "  - 이해하기 쉬운 학습 결과\n",
    "  - 누락된 값 있어도 처리 가능\n",
    "  - 분류와 무관한 특징도 처리 가능\n",
    "\n",
    "\n",
    "* 단점\n",
    "  - 과적합(overfitting)되기 쉬움: 너무 복잡한 의사결정 트리\n",
    "\n",
    "\n",
    "* 적용\n",
    "  - 수치형 값, 명목형 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tree construction\n",
    "\n",
    "### ID3 알고리즘\n",
    "1. 데이터를 가장 잘 나눌 수 있는 특징을 먼저 찾아서 데이터 집합을 하위 집합으로 분할\n",
    "  - 정보 이득(Information Gain)이 가장 큰 특징\n",
    "  - 엔트로피(Entopy)가 가장 크게 낮아지는 특징\n",
    "2. 해당 특징을 포함하는 노드 생성\n",
    "3. 하위 집합의 모든 데이터가 같은 클래스에 속하면 해당 하위 집합에 대한 분류 종료\n",
    "4. 2의 경우가 아니라면 이 하위 집합에 대해 1을 적용\n",
    "5. 모든 데이터가 분류될 때까지(= 모든 하위 집합에 대해) 1~4 반복\n",
    "  - 재귀적 방법으로 해결\n",
    "  \n",
    "\n",
    "* https://en.wikipedia.org/wiki/ID3_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 트리 구조 생성 의사코드 p.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General approach to Dicision Tree\n",
    "1. Collect:\n",
    "  - 모든 방법\n",
    "2. Prepare:\n",
    "  - 명목형 값\n",
    "  - 연속형 값(수치형)은 양자화를 통해 이산형 값으로 변환\n",
    "3. Analyze:\n",
    "  - 모든 방법\n",
    "  - 트리를 구성한 후 시각적으로 검토\n",
    "4. Train:\n",
    "  - 트리 데이터 구조를 구성\n",
    "5. Test: \n",
    "  - 학습된 트리로 오류율(error rate) 계산\n",
    "6. Use:\n",
    "  - 모든 지도학습에 사용 가능\n",
    "  - 대개 데이터를 더 잘 이해하기 위해 사용\n",
    "\n",
    "\n",
    "* 양자화(Quantization)\n",
    "  - https://ko.wikipedia.org/wiki/%EC%96%91%EC%9E%90%ED%99%94_(%EC%A0%95%EB%B3%B4_%EC%9D%B4%EB%A1%A0)\n",
    "  - http://www.ktword.co.kr/abbr_view.php?m_temp1=911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정 트리 알고리즘\n",
    "\n",
    "  - https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95\n",
    "  * ID3 (Iterative Dichotomiser 3)\n",
    "  * C4.5 (successor of ID3)\n",
    "  * C5.0 (successor of ID4)\n",
    "  * CART (Classification And Regression Tree)\n",
    "  * CHAID (CHi-squared Automatic Interaction Detector)\n",
    "  * MARS (Multivariate adaptive regression splines)\n",
    "  * 조건부 추론 트리 (Conditional Inference Trees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가정 적합한 분할 기준을 선택하는 방법\n",
    "  * 정보 이득\n",
    "  * 지니 불순도(Gini Impurity)\n",
    "  * 분산 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표 3.1 해양 동물 데이터 p.52\n",
    "  * '물고기이다'와 '물고기가 아니다'로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Information gain\n",
    "  * 데이터를 분할하기 이전과 이후의 정보량(엔트로피) 변화\n",
    "  * 정보 이득이 가장 큰 특징에 대해 분할 수행\n",
    "  * 정보 이득으로 정보의 불확실성(엔트로피) 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개별 정보량과 엔트로피 (p.53~54)\n",
    "\n",
    "  * 개별 정보량\n",
    "    * 확률이 낮을수록 개별 정보량은 커진다 == 엔트로피가 커지는데 기여\n",
    "      - 로그의 결과에 -1을 곱한 이유\n",
    "    * 밑이 2\n",
    "      - 정보를 전달(표현)하는데 몇 자리 2진수(몇 비트)면 충분한가\n",
    "\n",
    "\n",
    "  * 엔트로피\n",
    "    * 정보에 대한 기댓값\n",
    "    * 불확실한 정도, 무질서 정도\n",
    "    * 확률이 낮은 사건이 많을수록 정보의 엔트로피(불확실성)이 커진다\n",
    "    * 정보의 불확실성(엔트로피)가 높다\n",
    "      - 어떤 값(정보)가 나올 지 알기 힘들다\n",
    "    * 엔트로피가 높은 원인\n",
    "      - 모든 사건의 확률이 균등하다\n",
    "      - 확률이 낮은 사건이 많다\n",
    "        - 정보가 다양하다\n",
    "    \n",
    "\n",
    "\n",
    "* http://leosworld.tistory.com/8\n",
    "* https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.1 Function to calculate the Shannon entropy of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2 Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.2 Dataset splitting on a given feature\n",
    "  - dataSet: 분할하고자 하는 데이터 집합\n",
    "  - axis: 특징의 인덱스\n",
    "  - value: 특징의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "reducedFeatVec = featVec[:axis]     #chop out axis used for splitting\n",
    "reducedFeatVec.extend(featVec[axis+1:])\n",
    "\n",
    "# 2)\n",
    "reducedFeatVec = list(featVec)\n",
    "del reducedFeatVec[axis]\n",
    "\n",
    "# 3)\n",
    "reducedFeatVec = featVec[:axis] + featVec[axis+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.3 Choosing the best feature to split on\n",
    "  - Listing 3.1과 Listing 3.2의 함수 호출\n",
    "  \n",
    "* 가정\n",
    "  1. 데이터가 다중(중첩) 리스트\n",
    "  2. 데이터의 마지막 컬럼, 마지막 아이템이 클래스 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 분할하기전 엔트로피\n",
    "  * 0.9709505944546686\n",
    "\n",
    "### 0번 특징으로 분할\n",
    "  * 0번 특징이 0인 그룹 ==> 'no' 2개 ==> 정보가 없다\n",
    "    - 엔트로피: 0\n",
    "  * 0번 특징이 1인 그룹 ==> 'yes' 2개, 'no' 1개 ==> 확률이 2/3, 1/3\n",
    "    - 엔트로피: 0.9182958340544896\n",
    "  * 0번 특징으로 분할된 두 그룹에 대한 엔트로피의 기댓값\n",
    "    - 2/5 \\* 0.0 + 3/5 * 0.9182958340544896 = 0.5509775004326937\n",
    "  \n",
    "### 1번 특징으로 분할\n",
    "  * 1번 특징이 0인 그룹 ==> 'no' 1개 ==> 정보가 없다\n",
    "    - 엔트로피: 0\n",
    "  * 1번 특징이 1인 그룹 ==> 'yes' 2개, 'no' 2개 ==> 확률이 1/2, 1/2\n",
    "    - 엔트로피: 1.0\n",
    "  * 1번 특징으로 분할된 두 그룹에 대한 엔트로피의 기댓값\n",
    "    - 1/5 \\* 0.0 + 4/5 * 1.0 = 0.8\n",
    "    \n",
    "\n",
    "### ==> 0번 특징으로 분할 시 정보 이득이 더 크다\n",
    "### ==> 0번 특징이 최선의 분할 특징으로 선택된 것이 일리있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Recursively building the tree\n",
    "\n",
    "  1. 최선의 분할 특징으로 데이터 집합을 분할\n",
    "  2. 이진 트리가 아니므로 2개 이상으로도 분할 가능\n",
    "  3. 브랜치를 따라 하위 노드로 이동\n",
    "  4. 1~3의 과정을 반복 ==> 재귀적 호출\n",
    "  \n",
    "  \n",
    "  * 재귀적 호출에 대한 재귀 중단 조건\n",
    "    - 브랜치의 모든 사례가 같은 레이블일 경우\n",
    "    - 분할할 특징이 더 이상 남아있지 않은 경우\n",
    "      + 레이블 중에서 다수결에 의해 결정\n",
    "      + 다수결 코드 (p.61)\n",
    "        \n",
    "        \n",
    "  * 재귀가 중단된 지점의 노드\n",
    "    - 리프(leaf) 노드, 말단 블록(terminating block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 의사결정 트리를 생성하는 것이 학습하는 것\n",
    "##### Listing  3.4 Tree-building code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Plotting trees in Python with Matplotlib annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Matplotlib annotations\n",
    "* Parts of a Figure in matplotlib\n",
    "  - http://matplotlib.org/faq/usage_faq.html#parts-of-a-figure\n",
    "  - https://i.stack.imgur.com/HZWkV.png\n",
    "\n",
    "\n",
    "* matplotlib Annotation\n",
    "  - https://matplotlib.org/users/annotations.html\n",
    "    \n",
    "  - annotate() method\n",
    "    \n",
    "  - 두 개의 점 고려, 각 점은 튜플(x, y)로 표현됨\n",
    "    - xy: 애노테이션이 적용될 위치\n",
    "    - xytext: 출력할 문자 위치\n",
    "        \n",
    "  - 좌표계\n",
    "    + figure 계열: points, pixel, fraction\n",
    "    + axes 계열: points, pixel, fraction\n",
    "    + data: axes data coordinate system (기본값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.5 Plotting tree nodes with text annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Constructing a tree of annotations \n",
    "* 리프 노드의 개수 - 각 요소의 적절한 X축 방향 크기 계산에 필요\n",
    "* 트리의 깊이 - 각 요소의 적절한 Y축 방향 크기 계산에 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.6 Identifying the number of leaves in a tree and the depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.7 The plotTree function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Testing and storing the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 Test: using the tree for classification\n",
    "* 학습이 끝난 의사결정 트리를 분류 문제에 적용\n",
    "\n",
    "* 리프 노드를 만날 때까지 의사결정 트리를 순회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 3.8 Classification function for an existing decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Use: persisting the decision tree\n",
    "* 분류 한 문제마다 의사결정 트리를 작성하는 것은 시간 낭비\n",
    "* 훈련된 의사결정 트리를 저장해두었다가 필요할 때 불러내어 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # pickle 모듈\n",
    "- https://docs.python.org/3/library/pickle.html\n",
    "\n",
    "* 파이썬 객체 구조를 직렬화/역직렬화하기 위한 바이너리 프로토콜 구현\n",
    "\n",
    "\n",
    "* pickling: 파이썬 객체 ==> 바이트 스트림\n",
    "  - serialization, marshalling, flattening\n",
    "* unpickling: 바이트 스트림 ==> 파이썬 객체\n",
    "\n",
    "\n",
    "* pickle.dump()\n",
    "* pickle.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Example: using decision trees to predict contact lens type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect:\n",
    "  - 제공되는 텍스트 파일\n",
    "2. Prepare:\n",
    "  - TSV 파일을 파싱\n",
    "3. Analyze:\n",
    "  - 파싱된 데이터를 빠르게 눈으로 살펴본다.\n",
    "  - 트리를 그려서 확인한다: createPlot()\n",
    "4. Train:\n",
    "  - 트리 데이터 구조를 구성: createTree()\n",
    "5. Test: \n",
    "  - 주어진 사례를 트리에 적용하기 위한 함수 작성\n",
    "6. Use:\n",
    "  - 트리 데이터 구조가 저장소(파일, 데이터베이스 등)에 저장\n",
    "  - 필요시 트리 구조를 불러다 모든 애플리케이션에 사용 가능  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 렌즈 데이터 집합\n",
    "  * lenses.txt 데이터 파일\n",
    "  \n",
    "  \n",
    "  * age\n",
    "  * prescript\n",
    "  * astigmatic (난시)\n",
    "  * tear rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합(overfitting) 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
